{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb417da",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "## Tensor의 특징\n",
    "- NN에서 가장 중요한 연산은 행렬간의 곱이라 생각한다.\n",
    "- matrix연산을 수행하는 모듈로 Numpy module이 있는데, numpy의 array가 ndarray인데 이를 gpu에서 사용가능하게 셋팅한게 Tensor라 이해하면 편하다.\n",
    "- 실제로 tensor는 ndarray와 매우 유사하며, 메모리를 서로 공유할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "109413c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc98032",
   "metadata": {},
   "source": [
    "## Tensor의 초기화\n",
    "- 텐서의 초기화 방법은 크게 3가지가 있다.\n",
    "  - 데이터로부터 직접 생성, numpy array로부터 생성, 다른 텐서로부터 생성이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff04fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.7000, 0.2097],\n",
      "        [0.7868, 0.0407]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(x_np)\n",
    "\n",
    "x_ones = torch.ones_like(x_data) # *_like 메서드들은 속성(shape, dtype, device)을 카피한다\n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # 이게 dtype도 카피를 하는거같은데, rand같은 경우는 int타입에서 에러떠서 float로 정의\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89e405",
   "metadata": {},
   "source": [
    "## Tensor의 ones, zeros, rand\n",
    "- torch는 numpy처럼 ones, zeros, rand를 제공한다.\n",
    "- 참고로 ones_, zeros_, rand_처럼 맨뒤에 _ 가 붙은 메소드는 덮어쓰기를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf4cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6796, 0.4683, 0.3852],\n",
      "        [0.5754, 0.3788, 0.4945]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "\n",
    "ones_tensor = torch.ones(shape)\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25acd30",
   "metadata": {},
   "source": [
    "## 텐서의 속성\n",
    "- shape, dtype, device가 핵심 속성이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666d8da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0d9ee",
   "metadata": {},
   "source": [
    "## 텐서 연산\n",
    "- torch 모듈에 다양한 연산들이 있는데 이건 따로 공부를 해보도록 하자\n",
    "- 텐서는 기본적으로 CPU에 생성되는데 .to('cuda') 메서드를 실행하면 gpu에서 연산하게끔 할 수 있다.\n",
    "  - 물론 gpu가 사용가능한지 확인부터 해야한다.\n",
    "  - 참고로 cpu에서 gpu로 옮기는 비용이 작지 않은것을 기억해두자\n",
    "- numpy처럼 slicing이 자유로운점은 매우 편하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456dd162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f510c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a723b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22a959d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1) # 여기서 dim이 가변하는 dim을 하나 지정하는 것이다.\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a436be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산합니다. y1, y2, y3은 모두 같은 값을 갖습니다.\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "print(y1, y2, y3)\n",
    "\n",
    "# 요소별 곱(element-wise product)을 계산합니다. z1, z2, z3는 모두 같은 값을 갖습니다.\n",
    "# 내적아니다. 요소별 곱이다. 체크해둔다.\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "print(z1, z2, z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "776e3c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n",
      "tensor([[ 7.,  7.,  9., 10.],\n",
      "        [ 7.,  7.,  9., 10.],\n",
      "        [ 7.,  7.,  9., 10.],\n",
      "        [ 7.,  7.,  9., 10.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "tensor.add_(torch.tensor([1,2,3,4]))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f88ed758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "l: [1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "l = n.tolist()\n",
    "print(f\"l: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecec72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
